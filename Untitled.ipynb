{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3cf3bd4",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Advanced-Tokenization,-Stemming,-and-Lemmatization\" data-toc-modified-id=\"Advanced-Tokenization,-Stemming,-and-Lemmatization-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Advanced Tokenization, Stemming, and Lemmatization</a></span></li><li><span><a href=\"#Latent-Dirichlet-Allocation\" data-toc-modified-id=\"Latent-Dirichlet-Allocation-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Latent Dirichlet Allocation</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21133cc",
   "metadata": {},
   "source": [
    "# Advanced Tokenization, Stemming, and Lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47746d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "# load spacy's English-language models\n",
    "en_nlp = spacy.load('en')\n",
    "# instantiate nltk's Porter stemmer\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "# define function to compare lemmatization in spacy with stemming in nltk\n",
    "def compare_normalization(doc):\n",
    "# tokenize document in spacy\n",
    "doc_spacy = en_nlp(doc)\n",
    "# print lemmas found by spacy\n",
    "print(\"Lemmatization:\")\n",
    "print([token.lemma_ for token in doc_spacy])\n",
    "# print tokens found by Porter stemmer\n",
    "print(\"Stemming:\")\n",
    "print([stemmer.stem(token.norm_.lower()) for token in doc_spacy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1393666d",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_normalization(u\"Our meeting today was worse than yesterday, \"\n",
    "\"I'm scared of meeting the clients tomorrow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d68e87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Technicality: we want to use the regexp-based tokenizer\n",
    "# that is used by CountVectorizer and only use the lemmatization\n",
    "# from spacy. To this end, we replace en_nlp.tokenizer (the spacy tokenizer)\n",
    "# with the regexp-based tokenization.\n",
    "import re\n",
    "# regexp used in CountVectorizer\n",
    "regexp = re.compile('(?u)\\\\b\\\\w\\\\w+\\\\b')\n",
    "# load spacy language model and save old tokenizer\n",
    "en_nlp = spacy.load('en')\n",
    "old_tokenizer = en_nlp.tokenizer\n",
    "# replace the tokenizer with the preceding regexp\n",
    "en_nlp.tokenizer = lambda string: old_tokenizer.tokens_from_list(\n",
    "regexp.findall(string))\n",
    "# create a custom tokenizer using the spacy document processing pipeline\n",
    "# (now using our own tokenizer)\n",
    "def custom_tokenizer(document):\n",
    "doc_spacy = en_nlp(document, entity=False, parse=False)\n",
    "return [token.lemma_ for token in doc_spacy]\n",
    "# define a count vectorizer with the custom tokenizer\n",
    "lemma_vect = CountVectorizer(tokenizer=custom_tokenizer, min_df=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b411262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform text_train using CountVectorizer with lemmatization\n",
    "X_train_lemma = lemma_vect.fit_transform(text_train)\n",
    "print(\"X_train_lemma.shape: {}\".format(X_train_lemma.shape))\n",
    "# standard CountVectorizer for reference\n",
    "vect = CountVectorizer(min_df=5).fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "print(\"X_train.shape: {}\".format(X_train.shape))\n",
    "346 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d324296",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lemma.shape: (25000, 21596)\n",
    "X_train.shape: (25000, 27271)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bb05f3",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09948767",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(max_features=10000, max_df=.15)\n",
    "X = vect.fit_transform(text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd167edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_topics=10, learning_method=\"batch\",\n",
    "max_iter=25, random_state=0)\n",
    "# We build the model and transform the data in one step\n",
    "# Computing transform takes some time,\n",
    "# and we can save time by doing both at once\n",
    "document_topics = lda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7145c7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d74d234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each topic (a row in the components_), sort the features (ascending)\n",
    "# Invert rows with [:, ::-1] to make sorting descending\n",
    "sorting = np.argsort(lda.components_, axis=1)[:, ::-1]\n",
    "# Get the feature names from the vectorizer\n",
    "feature_names = np.array(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3196c720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the 10 topics:\n",
    "mglearn.tools.print_topics(topics=range(10), feature_names=feature_names,\n",
    "sorting=sorting, topics_per_chunk=5, n_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6223da",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda100 = LatentDirichletAllocation(n_topics=100, learning_method=\"batch\",\n",
    "max_iter=25, random_state=0)\n",
    "document_topics100 = lda100.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50484003",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = np.array([7, 16, 24, 25, 28, 36, 37, 45, 51, 53, 54, 63, 89, 97])\n",
    "sorting = np.argsort(lda100.components_, axis=1)[:, ::-1]\n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "mglearn.tools.print_topics(topics=topics, feature_names=feature_names,\n",
    "sorting=sorting, topics_per_chunk=7, n_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3d02df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by weight of \"music\" topic 45\n",
    "music = np.argsort(document_topics100[:, 45])[::-1]\n",
    "# print the five documents where the topic is most important\n",
    "for i in music[:10]:\n",
    "# pshow first two sentences\n",
    "print(b\".\".join(text_train[i].split(b\".\")[:2]) + b\".\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef55a660",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 10))\n",
    "topic_names = [\"{:>2} \".format(i) + \" \".join(words)\n",
    "for i, words in enumerate(feature_names[sorting[:, :2]])]\n",
    "# two column bar chart:\n",
    "for col in [0, 1]:\n",
    "start = col * 50\n",
    "end = (col + 1) * 50\n",
    "ax[col].barh(np.arange(50), np.sum(document_topics100, axis=0)[start:end])\n",
    "ax[col].set_yticks(np.arange(50))\n",
    "ax[col].set_yticklabels(topic_names[start:end], ha=\"left\", va=\"top\")\n",
    "ax[col].invert_yaxis()\n",
    "ax[col].set_xlim(0, 2000)\n",
    "yax = ax[col].get_yaxis()\n",
    "yax.set_tick_params(pad=130)\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
